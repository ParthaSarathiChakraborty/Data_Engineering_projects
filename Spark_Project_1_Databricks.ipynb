{"cells":[{"cell_type":"code","source":["# Step 1: Check if all the required files are placed\n\n%fs ls dbfs:/FileStore/tables/Data"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"13ea3fc0-e826-4a3f-8f19-1e956b8b7bfd","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Create Dataframes for orders,order_items, and customers. \n#The CSV files we are using don't have a schema hence while creating the data frame we define the schema.\n\ncustomer_df=spark.read.csv('dbfs:/FileStore/tables/Data/customer/part_00000',schema=\"\"\"customer_id INT,customer_fname STRING,customer_lname STRING,customer_email STRING,customer_password STRING,customer_street STRING,customer_city STRING,customer_state STRING,customer_zipcode INT\"\"\")\n\norders_df=spark.read.csv('dbfs:/FileStore/tables/Data/orders/part_00000',schema=\"\"\"order_id INT,order_date DATE,order_customer_id INT,order_status STRING\"\"\")\n\norder_items_df=spark.read.csv('dbfs:/FileStore/tables/Data/order_items/part_00000',schema=\"\"\"order_item_id INT,order_item_order_id INT,order_item_product_id INT,order_item_quantity INT,order_item_subtotal FLOAT,order_item_product_price FLOAT\"\"\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3627ec45-d967-40d8-8c88-5f6473ec3bec","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# View DataFrame\norder_items_df.show(2)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0e585088-0d57-44b2-ad5c-b2a59a797cab","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Step 4: Join Our Tables into a new DataFrame(oreder_details) to create a Denormalized data frame.\n\n# Joining customers and orders table initially.\n\ncustomers_orders_df=customer_df.join(orders_df,customer_df['customer_id']==orders_df['order_customer_id'])\n\n#Project the required Data using SELECT clause.\n\ncustomers_orders_df.select('customer_id','order_id','order_date','order_status').orderBy('customer_id').show(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4284de45-8756-4d10-9d6c-556aff12bff2","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Consolidating order_id,order_date, and order_status to structure data type.\n\nfrom pyspark.sql.functions import struct\ncustomers_orders_df.select('customer_id',struct('order_id','order_date','order_status').alias('order_details')).orderBy('customer_id').show(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ce38b941-b37c-4b76-a274-a56916d63fac","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Generate an array of struct. field using order_details. Here we are grouping the customer_id and storing the order_details in form of an array.\n\ncustomer_order_struct=customers_orders_df.select('customer_id',struct('order_id','order_date','order_status').alias('order_details'))\nfrom pyspark.sql.functions import collect_list\nfinal_df=customer_order_struct.groupBy('customer_id').agg(collect_list('order_details').alias('order_details')).orderBy('customer_id')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c2f4523a-e25c-4650-bfcf-5a27c3548afd","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Step 5: Export Data Frame into a JSON File.\n\nfinal_df.coalesce(1).write.json('dbfs:/FileStore/tables/Data/final')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b6bbcff7-81dd-4af0-8e58-e7a695287b45","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Previously we had performed the Denormalization for orders and customers. Now we will perform for the entire three tables.\n\n## Joining the tables\n\ncustomer_details=customer_df. \\\njoin(orders_df,customer_df['customer_id']==orders_df['order_customer_id']). \\\njoin(order_items_df,orders_df['order_id']==order_items_df['order_item_order_id'])\n\n## Create a Denormalized Data Frame by combining all the required feild under order_detail as Structure Data Type.\n\ndenorm_df=customer_details. \\\nselect('customer_id','customer_fname','customer_lname','customer_email','order_id','order_date','order_status',struct('order_item_id','order_item_product_id','order_item_subtotal').alias('order_item_details')). \\\ngroupBy('customer_id','customer_fname','customer_lname','customer_email','order_id','order_date','order_status'). \\\nagg(collect_list('order_item_details').alias('order_item_details')). \\\norderBy('customer_id'). \\\nselect('customer_id','customer_fname','customer_lname','customer_email',struct('order_id','order_date','order_status','order_item_details').alias('order_details')). \\\ngroupBy('customer_id','customer_fname','customer_lname','customer_email'). \\\nagg(collect_list('order_details').alias('order_details')). \\\norderBy('customer_id')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f0683285-36b4-47c9-8cfa-1b77a88595ca","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Export Data Frame into a JSON File.\n\ndenorm_df.coalesce(1).write.json('dbfs:/FileStore/tables/Data/denorm')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e45cae5f-a3a6-44fe-aec7-80e4b9af1c88","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Now we have the required data to do our analysis. Now it's time to analyze the Denormalized data using Spark.\n\n# We shall perform the below analysis on our data\n\n# Get the Details of the order placed by the customer on 2014 January 1st\n# Compute the monthly customer Revenue\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c1c0e79e-18ce-420f-907f-80e0cf1db3d1","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Problem Statement — 1:\n# Read the Data Frame.\n\njson_df=spark.read.json('dbfs:/FileStore/tables/Data/denorm/part-00000-tid-4357456608139543307-49cdb4fe-37a2-4435-be01-b6711f29eb3d-211-1-c000.json')\njson_df.show(2)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4dc7758d-40b6-4697-895b-c07d331b77d8","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["json_df.select('customer_id','customer_fname',explode('order_details').alias('order_details')). \\\nfilter('order_details.order_date LIKE \"2014-01-01%\"'). \\\norderBy('customer_id'). \\\nselect('customer_id','customer_fname','order_details.order_id','order_details.order_date','order_details.order_status'). \\\nshow(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"803d34a8-43fb-4c8e-a7ec-63e02f463621","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Problem Statement — 2:\n# To calculate the monthly customer revenue we need to perform aggregations(SUM) on order_item_subtotal from the order_items table.\n# In our input data, we have wrapped all the details into a struct data type Hence it's time to flatten all the details.\n\nflatten=json_df.select('customer_id','customer_fname',explode('order_details').alias('order_details')). \\\nselect('customer_id','customer_fname',col('order_details.order_date').alias('order_date'),col('order_details.order_id').alias('order_id'),col('order_details.order_status').alias('order_status'),explode('order_details.order_item_details').alias('order_item_details')). \\\nselect('customer_id','customer_fname','order_date','order_id','order_status','order_item_details.order_item_id','order_item_details.order_item_product_id','order_item_details.order_item_subtotal')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f1813d4a-85b2-4bae-8741-18411e3db0d3","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# After flattening our data let's write the logic to get the monthly revenue\n\nfrom pyspark.sql.functions import to_date\nfrom pyspark.sql import Row\nfrom pyspark.sql.functions import sum as _sum\n\nflatten.select('customer_id','customer_fname',col(\"order_date\"),to_date(col(\"order_date\"),\"yyyy-MM-dd\").alias(\"order_date_converted\"),'order_status','order_item_subtotal'). \\\nfilter(\"order_status IN ('COMPLETE','CLOSED')\"). \\\ngroupBy('customer_id','customer_fname',date_format('order_date_converted','yyyy-MM').alias('order_month')). \\\nagg(_sum('order_item_subtotal').alias('Revenue')). \\\norderBy('order_month'). \\\nshow()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c6e3d2fe-7a5e-4324-b466-5eed3db6ebe7","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Spark_Project_1_Databricks","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":4270627939621441}},"nbformat":4,"nbformat_minor":0}
