{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP7YGthycs2MdHGE8KdGZwS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ParthaSarathiChakraborty/KaggleRepository/blob/main/Databricks_Predict_Ads_Clicks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIuqAZkm57r6"
      },
      "outputs": [],
      "source": [
        "%scala\n",
        "\n",
        "val csv = spark.read.option(\"inferSchema\",\"true\").option(\"header\", \"true\").csv(\"/FileStore/tables/advertising-1.csv\")\n",
        "\n",
        "csv.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%scala\n",
        "\n",
        "csv.printSchema()"
      ],
      "metadata": {
        "id": "9X6Cw1536OCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "-- Statistics of Data\n",
        "\n",
        "\n",
        "%scala\n",
        "\n",
        "csv.select(\"DailyTimeSpentonSite\", \"Age\", \"AreaIncome\", \"DailyInternetUsage\", \"AdTopicLine\", \"City\", \"Male\", \"Country\", \"Timestamp\", \"ClickedonAd\" ).describe().show()\n"
      ],
      "metadata": {
        "id": "X597KMT56XJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "-- Create Temporary View so we can perform Spark SQL on Data\n",
        "\n",
        "%scala\n",
        "\n",
        "csv.createOrReplaceTempView(\"AdsData\");"
      ],
      "metadata": {
        "id": "9Fj-HoxV6aMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "-- One Visualization to Rule Them All​\n",
        "\n",
        "%sql\n",
        "\n",
        "select * from AdsData"
      ],
      "metadata": {
        "id": "fhSW8nlO6nHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "-- Collecting all String Columns into an Array\n",
        "\n",
        "%scala\n",
        "\n",
        "var StringfeatureCol = Array(\"AdTopicLine\", \"City\", \"Country\", \"Timestamp\");"
      ],
      "metadata": {
        "id": "bBqYKy1P6pvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "-- Example of StringIndexer\n",
        "\n",
        "%scala\n",
        "\n",
        "import org.apache.spark.ml.feature.StringIndexer\n",
        "\n",
        "val df = spark.createDataFrame(\n",
        "  Seq((0, \"a\"), (1, \"b\"), (2, \"c\"), (3, \"a\"), (4, \"a\"), (5, \"c\"))\n",
        ").toDF(\"id\", \"category\")\n",
        "\n",
        "df.show()\n",
        "\n",
        "val indexer = new StringIndexer()\n",
        "  .setInputCol(\"category\")\n",
        "  .setOutputCol(\"categoryIndex\")\n",
        "\n",
        "val indexed = indexer.fit(df).transform(df)\n",
        "\n",
        "indexed.show()\n"
      ],
      "metadata": {
        "id": "IHs84YMy7GQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "/*\n",
        "Define the Pipeline​\n",
        "A predictive model often requires multiple stages of feature preparation.\n",
        "\n",
        "A pipeline consists of a series of transformer and estimator stages that typically prepare a DataFrame for modeling and then train a predictive model.\n",
        "\n",
        "In this case, you will create a pipeline with stages:\n",
        "\n",
        "A StringIndexer estimator that converts string values to indexes for categorical features\n",
        "A VectorAssembler that combines categorical features into a single vector\n",
        "*/\n",
        "\n",
        "\n",
        "%scala\n",
        "\n",
        "import org.apache.spark.ml.attribute.Attribute\n",
        "import org.apache.spark.ml.feature.{IndexToString, StringIndexer}\n",
        "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
        "\n",
        "val indexers = StringfeatureCol.map { colName =>\n",
        "  new StringIndexer().setInputCol(colName).setHandleInvalid(\"skip\").setOutputCol(colName + \"_indexed\")\n",
        "}\n",
        "\n",
        "val pipeline = new Pipeline()\n",
        "                    .setStages(indexers)      \n",
        "\n",
        "val AdsFinalDF = pipeline.fit(csv).transform(csv)"
      ],
      "metadata": {
        "id": "Vp9W_xzc7KvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "-- Print Schema to view String Columns are converted in to equivalent Numerical Columns\n",
        "\n",
        "%scala\n",
        "\n",
        "AdsFinalDF.printSchema()"
      ],
      "metadata": {
        "id": "U39zt4S-7TF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "/*\n",
        "Split the Data\n",
        "It is common practice when building machine learning models to split the source data, using some of it to train the model and reserving some to test the trained model. \n",
        "In this project, you will use 70% of the data for training, and reserve 30% for testing.\n",
        "*/\n",
        "\n",
        "%scala\n",
        "\n",
        "val splits = AdsFinalDF.randomSplit(Array(0.7, 0.3))\n",
        "val train = splits(0)\n",
        "val test = splits(1)\n",
        "val train_rows = train.count()\n",
        "val test_rows = test.count()\n",
        "println(\"Training Rows: \" + train_rows + \" Testing Rows: \" + test_rows)"
      ],
      "metadata": {
        "id": "MjlA2VV47XAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "-- Vector assembler \n",
        "\n",
        "\n",
        "%scala\n",
        "\n",
        "import org.apache.spark.ml.feature.VectorAssembler\n",
        "\n",
        "val assembler = new VectorAssembler().setInputCols(Array(\"DailyTimeSpentonSite\", \"Age\", \"AreaIncome\", \"DailyInternetUsage\", \"AdTopicLine_indexed\", \"City_indexed\", \"Male\", \"Country_indexed\", \"Timestamp_indexed\")).setOutputCol(\"features\")\n",
        "\n",
        "val training = assembler.transform(train).select($\"features\", $\"ClickedonAd\".alias(\"label\"))\n",
        "\n",
        "training.show(false)"
      ],
      "metadata": {
        "id": "8cEEMVLq7jNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "/*\n",
        "Train a Classification Model\n",
        "Next, you need to train a Classification model using the training data. To do this, create an instance of the LogisticRegression algorithm you want to use and \n",
        "use its fit method to train a model based on the training DataFrame. In this project, \n",
        "you will use a Logistic Regression Classifier algorithm – though you can use the same technique for any of the regression algorithms supported in the spark.ml API\n",
        "*/\n",
        "\n",
        "%scala\n",
        "import org.apache.spark.ml.classification.LogisticRegression\n",
        "\n",
        "val lr = new LogisticRegression().setLabelCol(\"label\").setFeaturesCol(\"features\").setMaxIter(10).setRegParam(0.3)\n",
        "val model = lr.fit(training)\n",
        "println (\"Model trained!\")"
      ],
      "metadata": {
        "id": "3WyWj69t7n1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "-- Prepare the Testing Data\n",
        "\n",
        "%scala\n",
        "\n",
        "val testing = assembler.transform(test).select($\"features\", $\"ClickedonAd\".alias(\"trueLabel\"))\n",
        "testing.show(false)"
      ],
      "metadata": {
        "id": "osYB8BxF7xgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "-- Test the Model\n",
        "\n",
        "%scala\n",
        "\n",
        "val prediction = model.transform(testing)\n",
        "val predicted = prediction.select(\"features\", \"prediction\", \"probability\", \"trueLabel\")\n",
        "predicted.show()"
      ],
      "metadata": {
        "id": "SK8NUc3571GZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "-- Evaluating a Model (We got 93% Accuracy)\n",
        "\n",
        "\n",
        "%scala\n",
        "\n",
        "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
        "\n",
        "\n",
        "val evaluator = new MulticlassClassificationEvaluator()\n",
        "  .setLabelCol(\"trueLabel\")\n",
        "  .setPredictionCol(\"prediction\")\n",
        "  .setMetricName(\"accuracy\")\n",
        "val accuracy = evaluator.evaluate(prediction)\n",
        "\n",
        "Output:\n",
        "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
        "evaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = MulticlassClassificationEvaluator: uid=mcEval_ebd948da765b, metricName=accuracy, metricLabel=0.0, beta=1.0, eps=1.0E-15\n",
        "accuracy: Double = 0.9377049180327869\n"
      ],
      "metadata": {
        "id": "8mNcVxW0771P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val tp = predicted.filter(\"prediction == 1 AND truelabel == 1\").count().toFloat\n",
        "val fp = predicted.filter(\"prediction == 1 AND truelabel == 0\").count().toFloat\n",
        "val tn = predicted.filter(\"prediction == 0 AND truelabel == 0\").count().toFloat\n",
        "val fn = predicted.filter(\"prediction == 0 AND truelabel == 1\").count().toFloat\n",
        "val metrics = spark.createDataFrame(Seq(\n",
        " (\"TP\", tp),\n",
        " (\"FP\", fp),\n",
        " (\"TN\", tn),\n",
        " (\"FN\", fn),\n",
        " (\"Precision\", tp / (tp + fp)),\n",
        " (\"Recall\", tp / (tp + fn)))).toDF(\"metric\", \"value\")\n",
        "metrics.show()"
      ],
      "metadata": {
        "id": "--QlxJ3y8AOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fObH7svV8ERm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}